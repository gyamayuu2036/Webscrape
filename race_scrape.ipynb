{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTables(response):\n",
    "    # HTTPステータスコードが200（成功）の場合のみ処理を続行\n",
    "    if response.getcode() == 200:\n",
    "        # HTMLをパースしてBeautifulSoupオブジェクトを作成\n",
    "        bs = BeautifulSoup(response, 'html.parser')\n",
    "        bs = bs.decode('UTF-8')\n",
    "        html_string = str(bs)\n",
    "        html_io = StringIO(html_string)\n",
    "\n",
    "        # テーブルデータを抽出\n",
    "        tables = pd.read_html(html_io)\n",
    "        df = tables[0]\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(f\"HTTPステータスコード {response.getcode()}: ページの取得に失敗しました\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#開催年\n",
    "years = [str(i).zfill(4) for i in range(2021, 2022)]\n",
    "#開催場所 01:札幌, 02:函館, 03:福島, 04:新潟, 05:東京, 06:中山, 07:中京, 08:京都, 09::阪神, 10:小倉\n",
    "places = [str(i).zfill(2) for i in range (5, 6)]\n",
    "#開催回\n",
    "times = [str(i).zfill(2) for i in range(1, 2)]\n",
    "#開催日\n",
    "days = [str(i).zfill(2) for i in range(1, 2)]\n",
    "#レースNo\n",
    "races = [str(i).zfill(2) for i in range(1, 13)]\n",
    "\n",
    "raceIdList = []\n",
    "for y in years:\n",
    "    for p in places:\n",
    "        for t in times:\n",
    "            for d in days:\n",
    "                for r in races:\n",
    "                    raceIdList.append(y + p + t + d + r)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m escapeList \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m (os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39m'\u001b[39m\u001b[39mrace_html.pkl\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m----> 6\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39mrace_html.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m     escapeList \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mraceId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maddEscapeList\u001b[39m(\u001b[39mid\u001b[39m :\u001b[39mstr\u001b[39m, ll :\u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\yamay\\.python\\Webscrape\\Webscrape\\Lib\\site-packages\\pandas\\io\\pickle.py:206\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    204\u001b[0m         \u001b[39m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mWarning\u001b[39;00m)\n\u001b[1;32m--> 206\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39;49mload(handles\u001b[39m.\u001b[39;49mhandle)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    208\u001b[0m     \u001b[39m# e.g.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[39m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[39m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39mreturn\u001b[39;00m pc\u001b[39m.\u001b[39mload(handles\u001b[39m.\u001b[39mhandle, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://db.netkeiba.com/race/'\n",
    "colName = ['raceId', 'htmlBytes']\n",
    "df = pd.DataFrame(columns=colName)\n",
    "escapeList = []\n",
    "\n",
    "# pickleファイルが存在するか確認し、データを読み込む\n",
    "if os.path.isfile('race_html.pkl'):\n",
    "    df = pd.read_pickle('race_html.pkl')\n",
    "    escapeList = df['raceId'].to_list()\n",
    "\n",
    "# ページネーション用の競走IDリストを生成する関数\n",
    "def addEscapeList(id: str, ll: list):\n",
    "    idAry = [id[0:4], id[4:6], id[6:8], id[8:10], id[10:12]]\n",
    "    for r in range(1, 13):\n",
    "        idAry[4] = str(r).zfill(2)\n",
    "        ll.append(''.join(idAry))\n",
    "    if idAry[3] == '01':\n",
    "        for d in range(2, 9):\n",
    "            idAry[3] = str(d).zfill(2)\n",
    "            ll = addEscapeList(''.join(idAry), ll)\n",
    "    if idAry[2] == '01':\n",
    "        for t in range(2, 9):\n",
    "            idAry[2] = str(t).zfill(2)\n",
    "            ll = addEscapeList(''.join(idAry), ll)\n",
    "\n",
    "    return ll\n",
    "\n",
    "# raceIdListに実際の競走IDのリストがあると仮定します\n",
    "# raceIdList = ...\n",
    "\n",
    "for raceId in tqdm(raceIdList):\n",
    "    try:\n",
    "        if raceId in escapeList:\n",
    "            continue\n",
    "        response = url + raceId\n",
    "        html = requests.get(response)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser')\n",
    "\n",
    "        if 'レース結果' in soup.text:\n",
    "            tmpDf = pd.DataFrame([[raceId, html.content]], columns=colName)  # 'colmuns'の誤りを修正\n",
    "            df = pd.concat([df, tmpDf], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            escapeList = addEscapeList(raceId, escapeList)  # 変数名を修正\n",
    "\n",
    "        time.sleep(3)\n",
    "    except Exception as e:  # 例外をキャッチしてログに記録\n",
    "        print(f'例外が発生しました: {str(e)}')\n",
    "\n",
    "# DataFrameをpickleファイルに保存\n",
    "df.to_pickle('race_html.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv',encoding='utf-8')\n",
    "df.columns\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
